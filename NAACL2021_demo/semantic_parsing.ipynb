{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934b74e1",
   "metadata": {},
   "source": [
    "# NAACL'21 DLG4NLP Tutorial Demo: Semantic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7adcc",
   "metadata": {},
   "source": [
    "Tutorial link: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c3b2e",
   "metadata": {},
   "source": [
    "In this tutorial demo, we will use the Graph4NLP library to build a GNN-based semantic parsing model. The model consists of \n",
    "- graph construction module (e.g., node embedding based dynamic graph)\n",
    "- graph embedding module (e.g., Bi-Sep GAT)\n",
    "- predictoin module (e.g., RNN decoder with attention, copy and coverage mechanisms)\n",
    "\n",
    "We will use the built-in Graph2Seq model APIs to build the model, and evaluate it on the Jobs dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac004ea7",
   "metadata": {},
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de88301",
   "metadata": {},
   "source": [
    "1. Create virtual environment\n",
    "```\n",
    "conda create --name graph4nlp python=3.7\n",
    "conda activate graph4nlp\n",
    "```\n",
    "\n",
    "2. Install [graph4nlp](https://github.com/graph4ai/graph4nlp) library\n",
    "- Clone the github repo\n",
    "```\n",
    "git clone -b develop https://github.com/graph4ai/graph4nlp.git\n",
    "cd graph4nlp\n",
    "```\n",
    "- Then run `./configure` (or `./configure.bat` if you are using Windows 10) to config your installation. The configuration program will ask you to specify your CUDA version. If you do not have a GPU, please choose 'cpu'.\n",
    "```\n",
    "./configure\n",
    "```\n",
    "- Finally, install the package\n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "3. Set up StanfordCoreNLP (unnecessary for this demo because preprocessed data is provided)\n",
    "- Download [StanfordCoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
    "- Go to the root folder and start the server\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086ca306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph4nlp.pytorch.datasets.jobs import JobsDataset\n",
    "from graph4nlp.pytorch.models.graph2seq import Graph2Seq\n",
    "from graph4nlp.pytorch.models.graph2seq_loss import Graph2SeqLoss\n",
    "from graph4nlp.pytorch.modules.graph_construction import *\n",
    "from graph4nlp.pytorch.modules.evaluation.base import EvaluationMetricBase\n",
    "from graph4nlp.pytorch.modules.utils.config_utils import update_values\n",
    "from graph4nlp.pytorch.modules.utils.copy_utils import prepare_ext_vocab\n",
    "from graph4nlp.pytorch.modules.config import get_basic_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1637d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jobs:\n",
    "    def __init__(self, opt):\n",
    "        super(Jobs, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.use_copy = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"use_copy\"]\n",
    "        self.use_coverage = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"use_coverage\"]\n",
    "        self._build_device(self.opt)\n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_loss_function()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "\n",
    "    def _build_device(self, opt):\n",
    "        seed = opt[\"seed\"]\n",
    "        np.random.seed(seed)\n",
    "        if opt[\"use_gpu\"] != 0 and torch.cuda.is_available():\n",
    "            print('[ Using CUDA ]')\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            from torch.backends import cudnn\n",
    "            cudnn.benchmark = True\n",
    "            device = torch.device('cuda' if opt[\"gpu\"] < 0 else 'cuda:%d' % opt[\"gpu\"])\n",
    "        else:\n",
    "            print('[ Using CPU ]')\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "\n",
    "    def _build_dataloader(self):\n",
    "        if self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"graph_type\"] == \"dependency\":\n",
    "            topology_builder = DependencyBasedGraphConstruction\n",
    "            graph_type = 'static'\n",
    "            dynamic_init_topology_builder = None\n",
    "        elif self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"graph_type\"] == \"constituency\":\n",
    "            topology_builder = ConstituencyBasedGraphConstruction\n",
    "            graph_type = 'static'\n",
    "            dynamic_init_topology_builder = None\n",
    "        elif self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"graph_type\"] == \"node_emb\":\n",
    "            topology_builder = NodeEmbeddingBasedGraphConstruction\n",
    "            graph_type = 'dynamic'\n",
    "            dynamic_init_topology_builder = None\n",
    "        elif self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"graph_type\"] == \"node_emb_refined\":\n",
    "            topology_builder = NodeEmbeddingBasedRefinedGraphConstruction\n",
    "            graph_type = 'dynamic'\n",
    "            dynamic_init_graph_type = self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                \"dynamic_init_graph_type\"]\n",
    "            if dynamic_init_graph_type is None or dynamic_init_graph_type == 'line':\n",
    "                dynamic_init_topology_builder = None\n",
    "            elif dynamic_init_graph_type == 'dependency':\n",
    "                dynamic_init_topology_builder = DependencyBasedGraphConstruction\n",
    "            elif dynamic_init_graph_type == 'constituency':\n",
    "                dynamic_init_topology_builder = ConstituencyBasedGraphConstruction\n",
    "            else:\n",
    "                raise RuntimeError('Define your own dynamic_init_topology_builder')\n",
    "        else:\n",
    "            raise NotImplementedError(\"Define your topology builder.\")\n",
    "\n",
    "        dataset = JobsDataset(root_dir=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"root_dir\"],\n",
    "                              pretrained_word_emb_name=self.opt[\"pretrained_word_emb_name\"],\n",
    "                              pretrained_word_emb_url=self.opt[\"pretrained_word_emb_url\"],\n",
    "                              pretrained_word_emb_cache_dir=self.opt[\"pretrained_word_emb_cache_dir\"],\n",
    "                              val_split_ratio=self.opt[\"val_split_ratio\"],\n",
    "                              merge_strategy=self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                                  \"merge_strategy\"],\n",
    "                              edge_strategy=self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                                  \"edge_strategy\"],\n",
    "                              seed=self.opt[\"seed\"],\n",
    "                              word_emb_size=self.opt[\"word_emb_size\"],\n",
    "                              share_vocab=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                                  \"share_vocab\"],\n",
    "                              graph_type=graph_type,\n",
    "                              topology_builder=topology_builder,\n",
    "                              topology_subdir=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                                  \"topology_subdir\"],\n",
    "                              thread_number=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                                  \"thread_number\"],\n",
    "                              dynamic_graph_type=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                                  \"graph_type\"],\n",
    "                              dynamic_init_topology_builder=dynamic_init_topology_builder,\n",
    "                              dynamic_init_topology_aux_args=None)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset.train, batch_size=self.opt[\"batch_size\"], shuffle=True,\n",
    "                                           num_workers=1,\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        self.test_dataloader = DataLoader(dataset.test, batch_size=self.opt[\"batch_size\"], shuffle=False, num_workers=1,\n",
    "                                          collate_fn=dataset.collate_fn)\n",
    "        self.vocab = dataset.vocab_model\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = Graph2Seq.from_args(self.opt, self.vocab).to(self.device)\n",
    "\n",
    "    def _build_loss_function(self):\n",
    "        self.loss = Graph2SeqLoss(ignore_index=self.vocab.out_word_vocab.PAD,\n",
    "                                  use_coverage=self.use_coverage, coverage_weight=0.3)\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.opt[\"learning_rate\"])\n",
    "\n",
    "    def _build_evaluation(self):\n",
    "        self.metrics = [ExpressionAccuracy()]\n",
    "\n",
    "    def train(self):\n",
    "        max_score = -1\n",
    "        self._best_epoch = -1\n",
    "        for epoch in range(200):\n",
    "            self.model.train()\n",
    "            self.train_epoch(epoch, split=\"train\")\n",
    "            self._adjust_lr(epoch)\n",
    "            if epoch >= 0:\n",
    "                score = self.evaluate(split=\"test\")\n",
    "                if score >= max_score:\n",
    "                    print(\"Best model saved, epoch {}\".format(epoch))\n",
    "                    self.save_checkpoint(\"best.pth\")\n",
    "                    self._best_epoch = epoch\n",
    "                max_score = max(max_score, score)\n",
    "            if epoch >= 30 and self._stop_condition(epoch):\n",
    "                break\n",
    "        return max_score\n",
    "\n",
    "    def _stop_condition(self, epoch, patience=20):\n",
    "        return epoch > patience + self._best_epoch\n",
    "\n",
    "    def _adjust_lr(self, epoch):\n",
    "        def set_lr(optimizer, decay_factor):\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = group['lr'] * decay_factor\n",
    "\n",
    "        epoch_diff = epoch - self.opt[\"lr_start_decay_epoch\"]\n",
    "        if epoch_diff >= 0 and epoch_diff % self.opt[\"lr_decay_per_epoch\"] == 0:\n",
    "            if self.opt[\"learning_rate\"] > self.opt[\"min_lr\"]:\n",
    "                set_lr(self.optimizer, self.opt[\"lr_decay_rate\"])\n",
    "                self.opt[\"learning_rate\"] = self.opt[\"learning_rate\"] * self.opt[\"lr_decay_rate\"]\n",
    "                print(\"Learning rate adjusted: {:.5f}\".format(self.opt[\"learning_rate\"]))\n",
    "\n",
    "    def train_epoch(self, epoch, split=\"train\"):\n",
    "        assert split in [\"train\"]\n",
    "        print(\"Start training in split {}, Epoch: {}\".format(split, epoch))\n",
    "        loss_collect = []\n",
    "        dataloader = self.train_dataloader\n",
    "        step_all_train = len(dataloader)\n",
    "        for step, data in enumerate(dataloader):\n",
    "            graph, tgt, gt_str = data[\"graph_data\"], data[\"tgt_seq\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            oov_dict = None\n",
    "            if self.use_copy:\n",
    "                oov_dict, tgt = prepare_ext_vocab(graph, self.vocab, gt_str=gt_str, device=self.device)\n",
    "\n",
    "            prob, enc_attn_weights, coverage_vectors = self.model(graph, tgt, oov_dict=oov_dict)\n",
    "            loss = self.loss(logits=prob, label=tgt, enc_attn_weights=enc_attn_weights,\n",
    "                             coverage_vectors=coverage_vectors)\n",
    "            loss_collect.append(loss.item())\n",
    "            if step % self.opt[\"loss_display_step\"] == 0 and step != 0:\n",
    "                print(\"Epoch {}: [{} / {}] loss: {:.3f}\".format(epoch, step, step_all_train,\n",
    "                                                                           np.mean(loss_collect)))\n",
    "                loss_collect = []\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, split=\"val\"):\n",
    "        self.model.eval()\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        assert split in [\"val\", \"test\"]\n",
    "        dataloader = self.val_dataloader if split == \"val\" else self.test_dataloader\n",
    "        for data in dataloader:\n",
    "            graph, tgt, gt_str = data[\"graph_data\"], data[\"tgt_seq\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            if self.use_copy:\n",
    "                oov_dict = prepare_ext_vocab(batch_graph=graph, vocab=self.vocab, device=self.device)\n",
    "                ref_dict = oov_dict\n",
    "            else:\n",
    "                oov_dict = None\n",
    "                ref_dict = self.vocab.out_word_vocab\n",
    "\n",
    "            prob, _, _ = self.model(graph, oov_dict=oov_dict)\n",
    "            pred = prob.argmax(dim=-1)\n",
    "\n",
    "            pred_str = wordid2str(pred.detach().cpu(), ref_dict)\n",
    "            pred_collect.extend(pred_str)\n",
    "            gt_collect.extend(gt_str)\n",
    "\n",
    "        score = self.metrics[0].calculate_scores(ground_truth=gt_collect, predict=pred_collect)\n",
    "        print(\"Evaluation accuracy in `{}` split: {:.3f}\".format(split, score))\n",
    "        return score\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def translate(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        dataloader = self.test_dataloader\n",
    "        for data in dataloader:\n",
    "            graph, tgt, gt_str = data[\"graph_data\"], data[\"tgt_seq\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            if self.use_copy:\n",
    "                oov_dict = prepare_ext_vocab(batch_graph=graph, vocab=self.vocab, device=self.device)\n",
    "                ref_dict = oov_dict\n",
    "            else:\n",
    "                oov_dict = None\n",
    "                ref_dict = self.vocab.out_word_vocab\n",
    "\n",
    "            pred = self.model.translate(batch_graph=graph, oov_dict=oov_dict, beam_size=4, topk=1)\n",
    "\n",
    "            pred_ids = pred[:, 0, :]  # we just use the top-1\n",
    "\n",
    "            pred_str = wordid2str(pred_ids.detach().cpu(), ref_dict)\n",
    "\n",
    "            pred_collect.extend(pred_str)\n",
    "            gt_collect.extend(gt_str)\n",
    "\n",
    "        score = self.metrics[0].calculate_scores(ground_truth=gt_collect, predict=pred_collect)\n",
    "        return score\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_name):\n",
    "        checkpoint_path = os.path.join(self.opt[\"checkpoint_save_path\"], checkpoint_name)\n",
    "        self.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_name):\n",
    "        checkpoint_path = os.path.join(self.opt[\"checkpoint_save_path\"], checkpoint_name)\n",
    "        if not os.path.exists(self.opt[\"checkpoint_save_path\"]):\n",
    "            os.makedirs(self.opt[\"checkpoint_save_path\"], exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionAccuracy(EvaluationMetricBase):\n",
    "    def __init__(self):\n",
    "        super(ExpressionAccuracy, self).__init__()\n",
    "\n",
    "    def calculate_scores(self, ground_truth, predict):\n",
    "        correct = 0\n",
    "        assert len(ground_truth) == len(predict)\n",
    "        for gt, pred in zip(ground_truth, predict):\n",
    "            if gt == pred:\n",
    "                correct += 1.\n",
    "        return correct / len(ground_truth)\n",
    "\n",
    "def wordid2str(word_ids, vocab):\n",
    "    ret = []\n",
    "    assert len(word_ids.shape) == 2, print(word_ids.shape)\n",
    "    for i in range(word_ids.shape[0]):\n",
    "        id_list = word_ids[i, :]\n",
    "        ret_inst = []\n",
    "        for j in range(id_list.shape[0]):\n",
    "            if id_list[j] == vocab.EOS:\n",
    "                break\n",
    "            token = vocab.getWord(id_list[j])\n",
    "            ret_inst.append(token)\n",
    "        ret.append(\" \".join(ret_inst))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a702636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/graph4nlp/lib/python3.7/site-packages/graph4nlp-0.2a4-py3.7.egg/graph4nlp/pytorch/modules/utils/config_utils.py:32: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(setting)\n"
     ]
    }
   ],
   "source": [
    "# config setup\n",
    "config_file = 'config/jobs/gat_bi_sep_dynamic_node_emb.yaml'\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "opt = get_basic_args(graph_construction_name=config[\"graph_construction_name\"],\n",
    "                              graph_embedding_name=config[\"graph_embedding_name\"],\n",
    "                              decoder_name=config[\"decoder_name\"])\n",
    "update_values(to_args=opt, from_args_list=[config, config[\"other_args\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb15c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CPU ]\n",
      "Loading pre-built vocab model stored in data/jobs/processed/NodeEmbGraph/vocab.pt\n",
      "Start training in split train, Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/graph4nlp/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/Users/hugo/opt/anaconda3/envs/graph4nlp/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [10 / 21] loss: 3.846\n",
      "Epoch 0: [20 / 21] loss: 2.482\n",
      "Evaluation accuracy in `test` split: 0.000\n",
      "Best model saved, epoch 0\n",
      "Start training in split train, Epoch: 1\n",
      "Epoch 1: [10 / 21] loss: 1.832\n",
      "Epoch 1: [20 / 21] loss: 1.406\n",
      "Evaluation accuracy in `test` split: 0.000\n",
      "Best model saved, epoch 1\n",
      "Start training in split train, Epoch: 2\n",
      "Epoch 2: [10 / 21] loss: 1.223\n",
      "Epoch 2: [20 / 21] loss: 1.082\n",
      "Evaluation accuracy in `test` split: 0.000\n",
      "Best model saved, epoch 2\n",
      "Start training in split train, Epoch: 3\n",
      "Epoch 3: [10 / 21] loss: 1.033\n",
      "Epoch 3: [20 / 21] loss: 0.984\n",
      "Evaluation accuracy in `test` split: 0.057\n",
      "Best model saved, epoch 3\n",
      "Start training in split train, Epoch: 4\n",
      "Epoch 4: [10 / 21] loss: 0.935\n",
      "Epoch 4: [20 / 21] loss: 0.892\n",
      "Evaluation accuracy in `test` split: 0.121\n",
      "Best model saved, epoch 4\n",
      "Start training in split train, Epoch: 5\n",
      "Epoch 5: [10 / 21] loss: 0.901\n",
      "Epoch 5: [20 / 21] loss: 0.830\n",
      "Evaluation accuracy in `test` split: 0.050\n",
      "Start training in split train, Epoch: 6\n",
      "Epoch 6: [10 / 21] loss: 0.813\n",
      "Epoch 6: [20 / 21] loss: 0.774\n",
      "Evaluation accuracy in `test` split: 0.250\n",
      "Best model saved, epoch 6\n",
      "Start training in split train, Epoch: 7\n",
      "Epoch 7: [10 / 21] loss: 0.760\n",
      "Epoch 7: [20 / 21] loss: 0.722\n",
      "Evaluation accuracy in `test` split: 0.343\n",
      "Best model saved, epoch 7\n",
      "Start training in split train, Epoch: 8\n",
      "Epoch 8: [10 / 21] loss: 0.723\n",
      "Epoch 8: [20 / 21] loss: 0.659\n",
      "Evaluation accuracy in `test` split: 0.400\n",
      "Best model saved, epoch 8\n",
      "Start training in split train, Epoch: 9\n",
      "Epoch 9: [10 / 21] loss: 0.643\n",
      "Epoch 9: [20 / 21] loss: 0.679\n",
      "Evaluation accuracy in `test` split: 0.457\n",
      "Best model saved, epoch 9\n",
      "Start training in split train, Epoch: 10\n",
      "Epoch 10: [10 / 21] loss: 0.602\n",
      "Epoch 10: [20 / 21] loss: 0.628\n",
      "Evaluation accuracy in `test` split: 0.486\n",
      "Best model saved, epoch 10\n",
      "Start training in split train, Epoch: 11\n",
      "Epoch 11: [10 / 21] loss: 0.583\n",
      "Epoch 11: [20 / 21] loss: 0.567\n",
      "Evaluation accuracy in `test` split: 0.479\n",
      "Start training in split train, Epoch: 12\n",
      "Epoch 12: [10 / 21] loss: 0.595\n",
      "Epoch 12: [20 / 21] loss: 0.573\n",
      "Evaluation accuracy in `test` split: 0.521\n",
      "Best model saved, epoch 12\n",
      "Start training in split train, Epoch: 13\n",
      "Epoch 13: [10 / 21] loss: 0.557\n",
      "Epoch 13: [20 / 21] loss: 0.531\n",
      "Evaluation accuracy in `test` split: 0.586\n",
      "Best model saved, epoch 13\n",
      "Start training in split train, Epoch: 14\n",
      "Epoch 14: [10 / 21] loss: 0.513\n",
      "Epoch 14: [20 / 21] loss: 0.524\n",
      "Evaluation accuracy in `test` split: 0.579\n",
      "Start training in split train, Epoch: 15\n",
      "Epoch 15: [10 / 21] loss: 0.504\n",
      "Epoch 15: [20 / 21] loss: 0.488\n",
      "Evaluation accuracy in `test` split: 0.657\n",
      "Best model saved, epoch 15\n",
      "Start training in split train, Epoch: 16\n",
      "Epoch 16: [10 / 21] loss: 0.507\n",
      "Epoch 16: [20 / 21] loss: 0.493\n",
      "Evaluation accuracy in `test` split: 0.736\n",
      "Best model saved, epoch 16\n",
      "Start training in split train, Epoch: 17\n",
      "Epoch 17: [10 / 21] loss: 0.479\n",
      "Epoch 17: [20 / 21] loss: 0.467\n",
      "Evaluation accuracy in `test` split: 0.736\n",
      "Best model saved, epoch 17\n",
      "Start training in split train, Epoch: 18\n",
      "Epoch 18: [10 / 21] loss: 0.459\n",
      "Epoch 18: [20 / 21] loss: 0.489\n",
      "Evaluation accuracy in `test` split: 0.764\n",
      "Best model saved, epoch 18\n",
      "Start training in split train, Epoch: 19\n",
      "Epoch 19: [10 / 21] loss: 0.478\n",
      "Epoch 19: [20 / 21] loss: 0.444\n",
      "Evaluation accuracy in `test` split: 0.750\n",
      "Start training in split train, Epoch: 20\n",
      "Epoch 20: [10 / 21] loss: 0.451\n",
      "Epoch 20: [20 / 21] loss: 0.431\n",
      "Evaluation accuracy in `test` split: 0.771\n",
      "Best model saved, epoch 20\n",
      "Start training in split train, Epoch: 21\n",
      "Epoch 21: [10 / 21] loss: 0.464\n",
      "Epoch 21: [20 / 21] loss: 0.460\n",
      "Evaluation accuracy in `test` split: 0.771\n",
      "Best model saved, epoch 21\n",
      "Start training in split train, Epoch: 22\n",
      "Epoch 22: [10 / 21] loss: 0.423\n",
      "Epoch 22: [20 / 21] loss: 0.439\n",
      "Evaluation accuracy in `test` split: 0.743\n",
      "Start training in split train, Epoch: 23\n",
      "Epoch 23: [10 / 21] loss: 0.441\n",
      "Epoch 23: [20 / 21] loss: 0.412\n",
      "Evaluation accuracy in `test` split: 0.771\n",
      "Best model saved, epoch 23\n",
      "Start training in split train, Epoch: 24\n",
      "Epoch 24: [10 / 21] loss: 0.404\n",
      "Epoch 24: [20 / 21] loss: 0.423\n",
      "Evaluation accuracy in `test` split: 0.821\n",
      "Best model saved, epoch 24\n",
      "Start training in split train, Epoch: 25\n",
      "Epoch 25: [10 / 21] loss: 0.421\n",
      "Epoch 25: [20 / 21] loss: 0.399\n",
      "Evaluation accuracy in `test` split: 0.821\n",
      "Best model saved, epoch 25\n",
      "Start training in split train, Epoch: 26\n",
      "Epoch 26: [10 / 21] loss: 0.386\n",
      "Epoch 26: [20 / 21] loss: 0.435\n",
      "Evaluation accuracy in `test` split: 0.786\n",
      "Start training in split train, Epoch: 27\n",
      "Epoch 27: [10 / 21] loss: 0.440\n",
      "Epoch 27: [20 / 21] loss: 0.436\n",
      "Evaluation accuracy in `test` split: 0.793\n",
      "Start training in split train, Epoch: 28\n",
      "Epoch 28: [10 / 21] loss: 0.425\n",
      "Epoch 28: [20 / 21] loss: 0.419\n",
      "Evaluation accuracy in `test` split: 0.843\n",
      "Best model saved, epoch 28\n",
      "Start training in split train, Epoch: 29\n",
      "Epoch 29: [10 / 21] loss: 0.404\n",
      "Epoch 29: [20 / 21] loss: 0.380\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Best model saved, epoch 29\n",
      "Start training in split train, Epoch: 30\n",
      "Epoch 30: [10 / 21] loss: 0.389\n",
      "Epoch 30: [20 / 21] loss: 0.364\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Best model saved, epoch 30\n",
      "Start training in split train, Epoch: 31\n",
      "Epoch 31: [10 / 21] loss: 0.389\n",
      "Epoch 31: [20 / 21] loss: 0.362\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Best model saved, epoch 31\n",
      "Start training in split train, Epoch: 32\n",
      "Epoch 32: [10 / 21] loss: 0.382\n",
      "Epoch 32: [20 / 21] loss: 0.380\n",
      "Evaluation accuracy in `test` split: 0.786\n",
      "Start training in split train, Epoch: 33\n",
      "Epoch 33: [10 / 21] loss: 0.418\n",
      "Epoch 33: [20 / 21] loss: 0.455\n",
      "Evaluation accuracy in `test` split: 0.779\n",
      "Start training in split train, Epoch: 34\n",
      "Epoch 34: [10 / 21] loss: 0.415\n",
      "Epoch 34: [20 / 21] loss: 0.388\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Start training in split train, Epoch: 35\n",
      "Epoch 35: [10 / 21] loss: 0.372\n",
      "Epoch 35: [20 / 21] loss: 0.359\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Start training in split train, Epoch: 36\n",
      "Epoch 36: [10 / 21] loss: 0.355\n",
      "Epoch 36: [20 / 21] loss: 0.374\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Best model saved, epoch 36\n",
      "Start training in split train, Epoch: 37\n",
      "Epoch 37: [10 / 21] loss: 0.370\n",
      "Epoch 37: [20 / 21] loss: 0.374\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Best model saved, epoch 37\n",
      "Start training in split train, Epoch: 38\n",
      "Epoch 38: [10 / 21] loss: 0.368\n",
      "Epoch 38: [20 / 21] loss: 0.366\n",
      "Evaluation accuracy in `test` split: 0.879\n",
      "Best model saved, epoch 38\n",
      "Start training in split train, Epoch: 39\n",
      "Epoch 39: [10 / 21] loss: 0.344\n",
      "Epoch 39: [20 / 21] loss: 0.398\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Start training in split train, Epoch: 40\n",
      "Epoch 40: [10 / 21] loss: 0.392\n",
      "Epoch 40: [20 / 21] loss: 0.356\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 41\n",
      "Epoch 41: [10 / 21] loss: 0.339\n",
      "Epoch 41: [20 / 21] loss: 0.375\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Start training in split train, Epoch: 42\n",
      "Epoch 42: [10 / 21] loss: 0.345\n",
      "Epoch 42: [20 / 21] loss: 0.346\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 43\n",
      "Epoch 43: [10 / 21] loss: 0.337\n",
      "Epoch 43: [20 / 21] loss: 0.355\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Best model saved, epoch 43\n",
      "Start training in split train, Epoch: 44\n",
      "Epoch 44: [10 / 21] loss: 0.339\n",
      "Epoch 44: [20 / 21] loss: 0.344\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 45\n",
      "Epoch 45: [10 / 21] loss: 0.340\n",
      "Epoch 45: [20 / 21] loss: 0.339\n",
      "Evaluation accuracy in `test` split: 0.879\n",
      "Start training in split train, Epoch: 46\n",
      "Epoch 46: [10 / 21] loss: 0.333\n",
      "Epoch 46: [20 / 21] loss: 0.327\n",
      "Evaluation accuracy in `test` split: 0.907\n",
      "Start training in split train, Epoch: 47\n",
      "Epoch 47: [10 / 21] loss: 0.336\n",
      "Epoch 47: [20 / 21] loss: 0.328\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Best model saved, epoch 47\n",
      "Start training in split train, Epoch: 48\n",
      "Epoch 48: [10 / 21] loss: 0.324\n",
      "Epoch 48: [20 / 21] loss: 0.328\n",
      "Evaluation accuracy in `test` split: 0.879\n",
      "Start training in split train, Epoch: 49\n",
      "Epoch 49: [10 / 21] loss: 0.334\n",
      "Epoch 49: [20 / 21] loss: 0.330\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 50\n",
      "Epoch 50: [10 / 21] loss: 0.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Best model saved, epoch 50\n",
      "Start training in split train, Epoch: 51\n",
      "Epoch 51: [10 / 21] loss: 0.325\n",
      "Epoch 51: [20 / 21] loss: 0.322\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 52\n",
      "Epoch 52: [10 / 21] loss: 0.329\n",
      "Epoch 52: [20 / 21] loss: 0.316\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 53\n",
      "Epoch 53: [10 / 21] loss: 0.346\n",
      "Epoch 53: [20 / 21] loss: 0.360\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 54\n",
      "Epoch 54: [10 / 21] loss: 0.335\n",
      "Epoch 54: [20 / 21] loss: 0.366\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 55\n",
      "Epoch 55: [10 / 21] loss: 0.344\n",
      "Epoch 55: [20 / 21] loss: 0.336\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 56\n",
      "Epoch 56: [10 / 21] loss: 0.337\n",
      "Epoch 56: [20 / 21] loss: 0.311\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 57\n",
      "Epoch 57: [10 / 21] loss: 0.312\n",
      "Epoch 57: [20 / 21] loss: 0.316\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 58\n",
      "Epoch 58: [10 / 21] loss: 0.325\n",
      "Epoch 58: [20 / 21] loss: 0.321\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 59\n",
      "Epoch 59: [10 / 21] loss: 0.311\n",
      "Epoch 59: [20 / 21] loss: 0.314\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Best model saved, epoch 59\n",
      "Start training in split train, Epoch: 60\n",
      "Epoch 60: [10 / 21] loss: 0.326\n",
      "Epoch 60: [20 / 21] loss: 0.327\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Best model saved, epoch 60\n",
      "Start training in split train, Epoch: 61\n",
      "Epoch 61: [10 / 21] loss: 0.307\n",
      "Epoch 61: [20 / 21] loss: 0.304\n",
      "Evaluation accuracy in `test` split: 0.921\n",
      "Best model saved, epoch 61\n",
      "Start training in split train, Epoch: 62\n",
      "Epoch 62: [10 / 21] loss: 0.309\n",
      "Epoch 62: [20 / 21] loss: 0.298\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Start training in split train, Epoch: 63\n",
      "Epoch 63: [10 / 21] loss: 0.303\n",
      "Epoch 63: [20 / 21] loss: 0.298\n",
      "Evaluation accuracy in `test` split: 0.907\n",
      "Start training in split train, Epoch: 64\n",
      "Epoch 64: [10 / 21] loss: 0.301\n",
      "Epoch 64: [20 / 21] loss: 0.297\n",
      "Evaluation accuracy in `test` split: 0.914\n",
      "Start training in split train, Epoch: 65\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "runner = Jobs(opt)\n",
    "max_score = runner.train()\n",
    "print(\"Train finish, best val score: {:.3f}\".format(max_score))\n",
    "runner.load_checkpoint(\"best.pth\")\n",
    "# runner.evaluate(\"test\")\n",
    "test_score = runner.translate()\n",
    "print('Final test accuracy: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb547d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
